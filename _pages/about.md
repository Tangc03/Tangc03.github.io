---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am a third-year undergraduate student at [Yuanpei College](https://yuanpei.pku.edu.cn/en/), [Peking University (PKU)]([https://www.pku.edu.cn/](https://english.pku.edu.cn/)), and a junior in [Zhi Class](https://zhi-class.ai/), [School of Electronics Engineering and Computer Science (EECS)](https://eecs.pku.edu.cn/en/), [Peking University (PKU)]([https://www.pku.edu.cn/](https://english.pku.edu.cn/)).

Currently, I am majoring in Artificial Intelligence. I am honored to be advised by [Prof. Yunhai Tong](https://scholar.google.com/citations?hl=zh-CN&user=T4gqdPkAAAAJ), [Dr. Xiangtai Li](https://lxtgh.github.io/) from Bytedance Seed (Tiktok), Singapore, and to collaborate with [Jianzong Wu](https://jianzongwu.github.io/).

My research interests include multi-modal learning and visual generative methods for image, video, and artistic creations.
<!-- I have published more than 100 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>). -->


# üî• News
 <!-- - *2025.07*: &nbsp;üéâüéâ [AADNet]() is accepted by SMC 2025! -->
 <!-- - *2025.04*: &nbsp;üéâüéâ [StyleCraft]() is accepted by ICIC 2025! -->
 - *2025.02*: &nbsp;üéâüéâ [DiffSensei](https://jianzongwu.github.io/projects/diffsensei) is accepted by CVPR 2025!

# üìù Publications 
<!-- DiffSensei -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR2025</div><img src='images/papers/DiffSensei.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation](https://arxiv.org/abs/2412.07589)
  
Jianzong Wu, **Chao Tang**, Jingbo Wang, Yanhong Zeng, Xiangtai Li, Yunhai Tong

[**Paper**](https://arxiv.org/abs/2412.07589)
|
[**Project**](https://jianzongwu.github.io/projects/diffsensei)
|
[**Code**](https://github.com/jianzongwu/DiffSensei)

- MangaZero is a new large-scale manga dataset containing 43K manga pages and 427K annotated panels.
- DiffSensei is the first model that can generate manga images with high-quality and controllable multiple characters with complex scenes.

</div>
</div>

<!-- GPT4o Report -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv Report</div><img src='images/papers/GPT4o_Report.jpeg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[An Empirical Study of GPT-4o Image Generation Capabilities](https://arxiv.org/abs/2504.05979)
  
Sixiang Chen, Jinbin Bai, Zhuoran Zhao, Tian Ye, Qingyu Shi, Donghao Zhou, Wenhao Chai, Xin Lin, Jianzong Wu, **Chao Tang**, Shilin Xu, Tao Zhang, Haobo Yuan, Yikang Zhou, Wei Chow, Linfeng Li, Xiangtai Li, Lei Zhu, Lu Qi

[**Paper**](https://arxiv.org/abs/2504.05979)
|
[**Project**](https://github.com/Ephemeral182/Empirical-Study-of-GPT-4o-Image-Gen)
|
[**Code**](https://github.com/Ephemeral182/Empirical-Study-of-GPT-4o-Image-Gen)

- This study benchmarks GPT-4o's image generation against leading models, revealing key strengths and limitations in its multimodal capabilities.

</div>
</div>

<!-- StyleCraft -->
<div style="display:none">
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICIC2025 Oral</div><img src='images/papers/StyleCraft.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[StyleCraft: High-Quality Arbitrary Style Transfer via Unified Content-Style Fusion]()
  
**Chao Tang**, Xinhai Chang

[**Paper(Coming Soon)**]()

</div>
</div>
</div>

<!-- AADNet -->
<div style="display:none">
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">SMC 2025</div><img src='images/papers/AADNet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[AADNet: A Human-Mind-Inspired Multi-Modal Framework for Object Concept Learning]()
  
**Chao Tang**, Xinhai Chang

[**Paper(Coming Soon)**]()

</div>
</div>
</div>

# üéñ Honors and Awards
- *2024* Zhi Class Scholarship 
- *2022* Peking University Freshman Scholarship 
- *2022* Outstanding Graduate Award of Shenzhen Middle School
- *2020, 2021* First Prize in Chinese Physics Olympiad (CPhO), Guangdong Province

# üìñ Educations
- *2022.09 - now*, Yuanpei College, Peking University
- *2022.09 - now*, Zhi Class, School of EECS, Peking University

# üíÅ‚Äç‚ôÇÔ∏è Services
- Reviewer: DeLTa@ICLR'25, FM-Wild@ICLR'25, IJCNN'25, CogSci'25
<!-- - Reviewer: DeLTa@ICLR'2025, FM-Wild@ICLR'2025, IJCNN'2025, CogSci'2025, ACL-SRW(Student Research Workshop)'2025, ICONIP'2025, PRCV2025 -->

# üó∫Ô∏è Visitors
<div style="width: 200px;"> <!-- ÂèØ‰ª•Ê†πÊçÆÈúÄË¶ÅË∞ÉÊï¥ÂÆΩÂ∫¶ÔºåÊØîÂ¶Ç150px„ÄÅ300pxÁ≠â -->
    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=uPei8bWb9abBe6c8TNkGmGbiSjNsaoSwdTYiiNWSgx4&cl=ffffff&w=a"></script>
</div>
